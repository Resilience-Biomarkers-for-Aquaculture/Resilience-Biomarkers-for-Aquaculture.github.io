{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Start Here","text":""},{"location":"#welcome-to-the-resilience-biomarkers-field-guide","title":"Welcome to the Resilience Biomarkers Field Guide","text":"<p>This field guide documents the real journey of discovering and validating biomarkers for stress resilience in oyster aquaculture using RNA-seq meta-analysis. Rather than presenting an idealized workflow, this guide shares what actually worked, what didn't, and why\u2014grounded in a year-long research effort analyzing multiple Crassostrea virginica (Eastern oyster) and Perkinsus marinus (parasite) RNA-seq datasets.</p>"},{"location":"#what-this-guide-is","title":"What This Guide Is","text":"<p>A Practical Chronicle</p> <p>This is not a theoretical textbook. It's a field guide built from:</p> <ul> <li>Real analysis decisions documented in 50+ notebook entries</li> <li>Actual pivots and failures tracked across 25+ GitHub issues</li> <li>Hard-won lessons about batch effects, leakage, and when methods break down</li> <li>A working 6-gene classifier that emerged from systematic iteration</li> </ul>"},{"location":"#what-youll-find-here","title":"What You'll Find Here","text":""},{"location":"#problem-framing","title":"Problem Framing","text":"<p>Understand what \"resilience\" means in this project, the datasets we're working with, and the biological and technical constraints that shaped our approach.</p>"},{"location":"#pipelines","title":"Pipelines","text":"<p>Two validated analysis paths:</p> <ol> <li>Stepwise Differential Abundance: Control vs. treated \u2192 resistant vs. sensitive</li> <li>Two-Step Classifier Pipeline: Reproducibility scoring \u2192 logistic regression</li> </ol> <p>Both with explicit guidance on normalization, validation (LOSO), and common pitfalls (batch effects, leakage).</p>"},{"location":"#year-in-review","title":"Year in Review","text":"<p>A chronological walk through the project from December 2024 to September 2025, highlighting:</p> <ul> <li>Major methodological pivots (integrated \u2192 post-integrated analysis)</li> <li>\"Big Lessons\" about data integration and trait definition</li> <li>When to abandon an approach vs. when to iterate</li> </ul>"},{"location":"#sources","title":"Sources","text":"<p>Complete index of the GitHub issues and notebook posts referenced throughout this guide, organized by theme for easy exploration.</p>"},{"location":"#glossary","title":"Glossary","text":"<p>Key terms and concepts defined in the context of this specific project.</p>"},{"location":"#how-to-use-this-guide","title":"How to Use This Guide","text":"<p>If you're new to biomarker discovery: Start with Problem Framing to understand the research context, then explore the Timeline to see how approaches evolved.</p> <p>If you're implementing a similar analysis: Jump to Pipelines for validated workflows, then check Validation &amp; Pitfalls for things that will bite you.</p> <p>If you're troubleshooting an issue: Use the Sources indices to find relevant discussions\u2014chances are we hit the same problem.</p> <p>If you're evaluating this approach: Read the \"Big Lessons\" in the Timeline to understand when and why methods were abandoned or refined.</p>"},{"location":"#core-finding-the-6-gene-classifier","title":"Core Finding: The 6-Gene Classifier","text":"<p>After extensive iteration, this project identified a 6-gene panel that successfully distinguishes tolerant from sensitive oyster phenotypes:</p> <ul> <li>Emerged from post-data integration approach (combining datasets 1 &amp; 5)</li> <li>Validated using Leave-One-Study-Out (LOSO) cross-validation</li> <li>Uses logistic regression to minimize feature set while maintaining accuracy</li> <li>See Two-Step Classifier for full methodology</li> </ul>"},{"location":"#related-resources","title":"Related Resources","text":"<ul> <li>Main Project Website - Jekyll site with publications, presentations, and updates</li> <li>Analysis Notebook - All notebook posts organized by date</li> <li>GitHub Repository - Code, data, and issue tracking</li> <li>Cvirg_Pmarinus_RNAseq Repo - Primary analysis repository with issues</li> </ul>"},{"location":"#attribution-citation","title":"Attribution &amp; Citation","text":"<p>This field guide is developed by Shelly A. Wanamaker as part of the Resilience Biomarkers for Aquaculture project.</p> <p>If you use methods or insights from this guide, please cite:</p> <p>Wanamaker, S.A. (2025). Resilience Biomarkers Field Guide. Resilience Biomarkers for Aquaculture Project. https://resilience-biomarkers-for-aquaculture.github.io/field-guide/</p> <p>This guide is actively maintained and reflects work through September 2025. Check the Timeline for the most recent updates.</p>"},{"location":"glossary/","title":"Glossary","text":"<p>Key terms and concepts used throughout this field guide, defined in the context of the biomarker discovery project.</p>"},{"location":"glossary/#a","title":"A","text":"<p>Batch Effects Non-biological variation in gene expression data arising from technical differences between experimental batches (e.g., different sequencing runs, library preparation dates, labs). In this project, study-specific effects consistently dominated trait effects, limiting the effectiveness of integrated analysis approaches.</p> <p>Biomarker A measurable molecular indicator (in this case, gene expression level) that distinguishes between biological states (tolerant vs. sensitive phenotypes). The project identified a 6-gene biomarker panel.</p>"},{"location":"glossary/#c","title":"C","text":"<p>Classifier A machine learning model that predicts categorical outcomes (phenotypes) based on input features (gene expression). This project used logistic regression with LASSO regularization to build a minimal 6-gene classifier.</p> <p>COMBAT A batch correction method that adjusts for systematic technical variation using empirical Bayes. Attempted in Issue #18 with limited success.</p> <p>Cross-Validation A technique for assessing how well a model generalizes to independent data by systematically holding out portions of data for testing. See: LOSO.</p>"},{"location":"glossary/#d","title":"D","text":"<p>DEG (Differentially Expressed Gene) A gene whose expression level differs significantly between two or more conditions (e.g., resistant vs. sensitive, control vs. treated).</p> <p>DESeq2 A widely-used R package for differential gene expression analysis from RNA-seq count data. Used throughout this project for identifying DEGs.</p> <p>Differential Abundance Analysis method to identify features (genes, transcripts) that differ in quantity/expression between experimental groups. This project primarily used the nf-core/differentialabundance pipeline.</p> <p>Directionality Consistency In the two-step classifier approach, the requirement that a gene's fold change direction (up or down) is the same across multiple datasets. Ensures reproducibility of effect direction.</p>"},{"location":"glossary/#f","title":"F","text":"<p>FastP A tool for quality control and preprocessing of FASTQ files (raw sequencing reads). Parameter selection is critical for different library types (e.g., TAG-seq vs. standard RNA-seq) - see Issue #26, #28.</p> <p>Fold Change The ratio of gene expression levels between two conditions, often expressed as log2 fold change. Positive values indicate upregulation, negative values indicate downregulation.</p>"},{"location":"glossary/#g","title":"G","text":"<p>GMT File Gene Matrix Transposed file format used for gene set definitions in GSEA. Contains pathway/gene set names with associated genes. Created in this project for GSEA integration.</p> <p>GSEA (Gene Set Enrichment Analysis) A computational method that determines whether a defined set of genes (e.g., a pathway) shows statistically significant differences between two biological states. Attempted in Issue #45.</p>"},{"location":"glossary/#h","title":"H","text":"<p>Heterogeneity In the context of gene scoring, the variance in gene expression within phenotype groups. Low heterogeneity (high within-group consistency) is desirable for biomarkers.</p>"},{"location":"glossary/#i","title":"I","text":"<p>Innate Biomarker A gene that is constitutively different in expression between resistant and sensitive individuals, even in the absence of stress. Contrasts with reactive biomarkers. Key insight from Issue #53.</p> <p>Integrated Data Analysis Approach that pools multiple datasets together before analysis, treating all samples as if from a single study (with batch correction). In this project, integrated analysis failed due to strong study-specific effects (Big Lesson #1).</p>"},{"location":"glossary/#l","title":"L","text":"<p>LASSO (Least Absolute Shrinkage and Selection Operator) A regularization method for regression that can shrink coefficients to exactly zero, performing automatic feature selection. Used in the two-step classifier to identify the minimal gene set.</p> <p>Leakage (Data Leakage) When information from the test set inappropriately influences the training process, leading to overly optimistic performance estimates. See Validation &amp; Pitfalls.</p> <p>LOSO (Leave-One-Study-Out) A cross-validation strategy where each study is held out once as a test set, while all other studies form the training set. Critical for assessing cross-study generalization (Big Lesson #4).</p>"},{"location":"glossary/#m","title":"M","text":"<p>Meta-Analysis Statistical approach that combines results from multiple independent studies to identify consistent patterns. This project used a post-data integration meta-analysis approach.</p> <p>Mutual Information A measure of statistical dependence between variables. Used early in the project to assess gene-phenotype associations, but trait separation was weak.</p>"},{"location":"glossary/#n","title":"N","text":"<p>nf-core A community effort to collect curated bioinformatics pipelines built using Nextflow. This project used nf-core/rnaseq and nf-core/differentialabundance.</p> <p>Normalization Process of adjusting gene expression data to account for technical variation (library size, sequencing depth, GC content) while preserving biological variation. Critical for cross-sample comparison.</p>"},{"location":"glossary/#p","title":"P","text":"<p>PCA (Principal Component Analysis) A dimensionality reduction technique that identifies major sources of variation in high-dimensional data (like gene expression). Used throughout the project to visualize sample clustering and assess batch effects.</p> <p>Perkinsus marinus Protozoan parasite that causes Dermo disease in oysters, the primary stressor in this project's datasets.</p> <p>Phenotype Observable trait or characteristic. In this project: tolerant/resistant vs. sensitive/susceptible oyster phenotypes in response to P. marinus infection.</p> <p>Post-Data Integration Approach that analyzes each dataset independently, then compares results across datasets to identify reproducible findings. This project pivoted to this approach after integrated analysis failed.</p>"},{"location":"glossary/#r","title":"R","text":"<p>Reactive Biomarker A gene whose differential expression between resistant and sensitive individuals emerges only after stress exposure (i.e., significant in treated samples but not controls). Contrasts with innate biomarkers.</p> <p>RemoveBatchEffect A function from the limma R package for adjusting gene expression data to remove batch effects. Attempted in Issue #18 with limited success.</p> <p>Reproducibility In the context of the two-step classifier, the degree to which a gene is identified as differentially expressed across multiple independent datasets. High reproducibility indicates robust, generalizable biomarkers.</p> <p>RNA-seq RNA sequencing - a high-throughput method for quantifying gene expression by sequencing cellular RNA. The primary data type in this project.</p>"},{"location":"glossary/#s","title":"S","text":"<p>Stepwise Differential Abundance A two-step filtering approach: (1) identify stress-responsive genes (control vs. treated), then (2) identify resistance-associated genes from Step 1 genes. Has limitations (removes innate biomarkers, breaks down with small gene sets) - see Stepwise Pipeline.</p> <p>Study-Specific Effects Systematic differences between datasets due to experimental design, protocols, or biological differences between populations. In this project, these effects were stronger than trait effects, driving the pivot to post-data integration.</p>"},{"location":"glossary/#t","title":"T","text":"<p>TAG-seq (3' Tag RNA-Sequencing) A cost-effective alternative to standard RNA-seq that sequences only the 3' end of transcripts. Requires different data processing parameters than full-length RNA-seq (see TAG-seq issues).</p> <p>Tolerant/Resistant Phenotype category for oysters that survived P. marinus infection or showed low infection intensity and minimal pathology.</p> <p>Trait Effects Gene expression differences attributable to the biological phenotype of interest (resistant vs. sensitive). In this project, trait effects were initially weaker than study-specific effects.</p>"},{"location":"glossary/#v","title":"V","text":"<p>Validation Process of confirming that a biomarker panel or model performs well on independent data not used during development. See Validation &amp; Pitfalls.</p> <p>VST (Variance-Stabilizing Transformation) A DESeq2 normalization method that transforms count data to a scale where variance is roughly constant across the range of expression values. Enables visualization and clustering. Requires sufficient genes (&gt;1000) for robust estimation.</p>"},{"location":"glossary/#abbreviations","title":"Abbreviations","text":"<ul> <li>DE: Differential Expression</li> <li>DEG: Differentially Expressed Gene</li> <li>FDR: False Discovery Rate (adjusted p-value threshold)</li> <li>GC Bias: Systematic bias related to GC content in sequencing</li> <li>GTF: Gene Transfer Format (gene annotation file)</li> <li>HPC: High-Performance Computing</li> <li>LOSO: Leave-One-Study-Out</li> <li>PCA: Principal Component Analysis</li> <li>QC: Quality Control</li> <li>WGBS: Whole-Genome Bisulfite Sequencing (for methylation)</li> </ul>"},{"location":"glossary/#species","title":"Species","text":"<ul> <li>Crassostrea virginica: Eastern oyster (primary study species)</li> <li>Crassostrea gigas: Pacific oyster (used for reference genome/annotation)</li> <li>Perkinsus marinus: Dermo disease-causing parasite</li> <li>Mytilus chilensis: Chilean blue mussel (methylation studies)</li> </ul> <p>Related Pages:</p> <ul> <li>Start Here - Field guide overview</li> <li>Problem Framing - Detailed background on resilience and datasets</li> <li>Timeline - See concepts in chronological context</li> </ul>"},{"location":"problem-framing/","title":"Problem Framing","text":""},{"location":"problem-framing/#what-is-resilience-in-this-project","title":"What is \"Resilience\" in This Project?","text":"<p>In the context of this research, resilience refers to the ability of Crassostrea virginica (Eastern oyster) to tolerate or resist stress from Perkinsus marinus (Dermo disease) infection and other environmental challenges.</p>"},{"location":"problem-framing/#defining-the-phenotype","title":"Defining the Phenotype","text":"<p>Throughout this project, phenotype definition evolved significantly:</p> <p>Big Lesson #2: Traits Were Oversimplified</p> <p>Early attempts used generalized trait definitions across studies (e.g., \"stress\" vs. \"control\"). This oversimplification failed because:</p> <ul> <li>Different studies measured different stressors (disease, temperature, salinity)</li> <li>Trait effects were weaker than study-specific effects</li> <li>Batch correction couldn't compensate for fundamental differences in experimental design</li> </ul> <p>Current approach: Focus on specific, comparable phenotypes:</p> <ul> <li>Tolerant/Resistant: Oysters that survived or showed low infection intensity</li> <li>Sensitive/Susceptible: Oysters that died or showed high infection intensity</li> <li>Control vs. Treated: Within-study comparisons before examining resilience</li> </ul>"},{"location":"problem-framing/#the-datasets","title":"The Datasets","text":"<p>This project integrates multiple RNA-seq datasets from C. virginica exposed to P. marinus:</p>"},{"location":"problem-framing/#primary-datasets","title":"Primary Datasets","text":"<ul> <li>Dataset 1: [Description of dataset 1 characteristics]</li> <li>Dataset 4: Injected group samples</li> <li>Dataset 5: [Description of dataset 5 characteristics]</li> <li>Additional datasets analyzed for specific questions</li> </ul>"},{"location":"problem-framing/#data-characteristics","title":"Data Characteristics","text":"<p>Common challenges:</p> <ul> <li>Batch effects: Study-specific effects consistently stronger than trait effects</li> <li>Sample size limitations: Variable n across studies</li> <li>Time point variation: Sampling at different post-exposure times</li> <li>Technology differences: Including TAG-seq vs. standard RNA-seq (issue #26, #28)</li> </ul>"},{"location":"problem-framing/#key-constraints-design-decisions","title":"Key Constraints &amp; Design Decisions","text":""},{"location":"problem-framing/#1-integration-vs-post-integration-analysis","title":"1. Integration vs. Post-Integration Analysis","text":"<p>Big Lesson #1: When Integrated Analysis Fails</p> <p>Integrated data analysis does not work when data is noisy (too much within and/or across study variation) and signal is not strong enough.</p> <p>Initial approach (Failed): - Pooled all datasets together - Attempted batch correction (COMBAT, RemoveBatchEffect) - Expected to see trait-based clustering</p> <p>Outcome: Study-specific effects dominated; trait separation was poor</p> <p>Pivot: Adopted post-data integration approach: 1. Analyze each dataset independently 2. Compare results across datasets 3. Identify reproducible signatures</p>"},{"location":"problem-framing/#2-fixed-vs-random-effects","title":"2. Fixed vs. Random Effects","text":"<p>Early attempts didn't properly account for:</p> <ul> <li>Sample size differences across studies</li> <li>Control group considerations</li> <li>Study as a random effect in mixed models</li> </ul>"},{"location":"problem-framing/#3-normalization-timing","title":"3. Normalization Timing","text":"<p>When Normalization Happens</p> <p>In the <code>nf-core/differentialabundance</code> pipeline:</p> <ul> <li>PCAs are generated before differential abundance analysis</li> <li>Normalization (VST, TPM) happens during analysis</li> <li>Batch correction (if applied) occurs on normalized counts</li> </ul> <p>Understanding this order is critical for interpreting preliminary QC plots</p>"},{"location":"problem-framing/#4-innate-vs-reactive-biomarkers","title":"4. Innate vs. Reactive Biomarkers","text":"<p>Big Lesson #3: Don't Filter Out Innate Signals</p> <p>Biomarkers may exist in control groups if they represent innate resilience traits (genes that are constitutively different in resistant vs. sensitive individuals).</p> <p>Implication: The stepwise approach (filter controls first) may inadvertently remove true biomarkers</p> <p>Resolution: Developed alternative classifier approach that preserves innate signals (see Two-Step Classifier)</p> <p>See issue #53 and notebook post for full innate vs. reactive analysis.</p>"},{"location":"problem-framing/#biological-context","title":"Biological Context","text":""},{"location":"problem-framing/#perkinsus-marinus-dermo","title":"Perkinsus marinus (Dermo)","text":"<ul> <li>Major pathogen in oyster aquaculture</li> <li>Causes dermo disease (mortality and reduced growth)</li> <li>Variable infection response across oyster populations</li> <li>Understanding resistance is key to breeding programs</li> </ul>"},{"location":"problem-framing/#molecular-signatures-of-resilience","title":"Molecular Signatures of Resilience","text":"<p>This project aims to identify gene expression patterns that:</p> <ol> <li>Predict tolerance/resistance phenotypes</li> <li>Are reproducible across independent studies</li> <li>Are biologically interpretable (pathway-informed)</li> <li>Could be assayable in breeding programs (minimal gene panels)</li> </ol>"},{"location":"problem-framing/#research-questions-evolution","title":"Research Questions Evolution","text":""},{"location":"problem-framing/#initial-questions-december-2024","title":"Initial Questions (December 2024)","text":"<ul> <li>Can we identify shared stress response genes across multiple studies?</li> <li>Do RNA-seq datasets cluster by trait when integrated?</li> </ul>"},{"location":"problem-framing/#refined-questions-january-april-2025","title":"Refined Questions (January-April 2025)","text":"<ul> <li>How do batch effects and study design differences limit integration?</li> <li>Can batch correction methods recover trait-based signal?</li> <li>Which normalization approaches work best for meta-analysis?</li> </ul>"},{"location":"problem-framing/#current-questions-august-september-2025","title":"Current Questions (August-September 2025)","text":"<ul> <li>What is the minimal gene set that distinguishes tolerant from sensitive oysters?</li> <li>Are biomarkers innate or reactive (expressed before vs. after stress)?</li> <li>Can classifiers trained on one study predict phenotypes in independent studies?</li> </ul>"},{"location":"problem-framing/#success-criteria","title":"Success Criteria","text":"<p>A successful biomarker panel should:</p> <p>\u2705 Show reproducible differential expression across datasets \u2705 Maintain predictive accuracy in Leave-One-Study-Out (LOSO) validation \u2705 Be small enough for practical assay development (&lt; 10 genes) \u2705 Have biological interpretability (annotatable, pathway-linked) \u2705 Distinguish phenotypes in both control and treated conditions  </p> <p>Next: Explore validated analysis approaches in Pipelines</p>"},{"location":"timeline/","title":"Year in Review: Timeline of Discovery","text":"<p>This timeline documents the evolution of analysis approaches from December 2024 through September 2025, highlighting methodological pivots, key lessons, and the path to a validated 6-gene classifier.</p>"},{"location":"timeline/#december-2024-initial-methods-exploration","title":"December 2024: Initial Methods Exploration","text":""},{"location":"timeline/#week-of-dec-4","title":"Week of Dec 4","text":"<p>Attempted: Running nf-core pipeline on 4 combined datasets</p> <p>Outcome: \u274c Failed - compute resource limits and too many unaccounted nuances</p> <p>Key Insight</p> <p>\"Too many nuances that can't be accounted for when everything is pooled\"</p> <p>Related:</p> <ul> <li>Notebook: Differential abundance workflow exploration</li> </ul>"},{"location":"timeline/#january-2025-differential-abundance-beginnings","title":"January 2025: Differential Abundance Beginnings","text":""},{"location":"timeline/#jan-3","title":"Jan 3","text":"<p>Activities:</p> <ul> <li>Issue #3: Created merged metadata</li> <li>Issue #4: Differential abundance initial approach</li> <li>Toy example: \u2705 Success</li> <li>GTF file issues encountered</li> <li>Applied mutual information to Perkinsus datasets</li> </ul> <p>Outcome: Merged counts table obtained, but separation by trait was weak</p> <p>Related:</p> <ul> <li>Notebook: <code>2025-01-03_RNAseq_all_AI_diffexp.ipynb</code></li> </ul>"},{"location":"timeline/#jan-16","title":"Jan 16","text":"<p>Attempted: Subset data to improve separation</p> <p>Outcome: \u274c Didn't help significantly</p> <p>Considerations identified:</p> <ul> <li>Fixed vs. random effects</li> <li>Sample size limitations</li> <li>Control group treatment</li> <li>Batch corrections needed</li> </ul> <p>Issues opened:</p> <ul> <li>Issue #9: Add study 5</li> <li>Issue #12: Decide best methods and execute</li> </ul> <p>Big Lesson #1: When Integration Fails</p> <p>Integrated data analysis does not work when data is noisy (too much within and/or across study variation) and signal is not strong enough.</p> <p>Pivot discussed: Meta-analysis approach (inspired by BMC paper with Erin Witkop, Dina co-author)</p>"},{"location":"timeline/#february-2025-confronting-batch-effects","title":"February 2025: Confronting Batch Effects","text":"<p>Key finding: Study-specific effects are much stronger than trait effects</p> <p>Attempted:</p> <ul> <li>Issue #18: Batch correction</li> <li>COMBAT</li> <li>RemoveBatchEffect</li> </ul> <p>Outcome: \u274c Little effect on improving trait-based separation</p> <p>Big Lesson #2: Traits Were Oversimplified</p> <p>Generalized trait definitions across studies were insufficient. Study-specific effects dominated.</p> <p>Alternative approach: Limit variation within study by subsetting for common time points, compare DEGs against control groups</p>"},{"location":"timeline/#april-2025-per-dataset-analysis-tag-seq-issues","title":"April 2025: Per-Dataset Analysis &amp; TAG-seq Issues","text":""},{"location":"timeline/#strategy-shift-independent-dataset-analysis","title":"Strategy Shift: Independent Dataset Analysis","text":"<p>New approach: Running DifferentialAbundance on each dataset independently</p>"},{"location":"timeline/#tag-seq-parameter-problems","title":"TAG-seq Parameter Problems","text":"<ul> <li>Issue #26: What flags to use? Discovered GC bias</li> <li>Critical finding: Johnson dataset used TAG-seq; initial RNA-seq analysis parameters were inappropriate</li> <li>Issue #28: Rerun Johnson data with different parameters</li> </ul> <p>Related:</p> <ul> <li>Notebook: Reprocess TAG-seq with FastP params</li> </ul>"},{"location":"timeline/#deferred-work","title":"Deferred Work","text":"<ul> <li>Issue #29 &amp; #31: Ran differential abundance on all datasets together but didn't interpret results yet</li> </ul> <p>Could return to this</p> <p>Results exist but interpretation was postponed to focus on per-dataset approach</p>"},{"location":"timeline/#june-2025-focused-dataset-analysis","title":"June 2025: Focused Dataset Analysis","text":""},{"location":"timeline/#per-dataset-deep-dives","title":"Per-Dataset Deep Dives","text":"<p>Issue #32: Attempted differentialabundance on datasets separately</p> <ul> <li>Steve: Focused on study 5</li> <li>Shelly: Focused on study 1</li> <li>Goal: Understand parameters and how to best run differential abundance</li> </ul>"},{"location":"timeline/#july-2025-combining-studies-understanding-normalization","title":"July 2025: Combining Studies &amp; Understanding Normalization","text":""},{"location":"timeline/#study-combination-experiment","title":"Study Combination Experiment","text":"<p>Issue #34: Combine study 4 (injected group) + study 5</p> <p>Research question: Will study 4 injected group cluster with resistance or susceptible group from study 5?</p> <p>Learning: Gained deeper understanding of the differentialabundance pipeline</p> <ul> <li>PCAs are generated before any differential abundance analysis happens</li> <li>Normalization timing matters for interpretation</li> </ul> <p>Batch correction attempt:</p> <ul> <li>Compared PCAs with and without batch correction on top 500 most variable genes</li> <li>Result: Starting to see evidence of innate trait</li> </ul> <p>Could return to this</p> <p>Analysis exists showing 567 genes with significant differential abundance (DESeq). Could revisit to see if these genes show greater clustering than the top 500 most variable.</p> <p>Related:</p> <ul> <li>Notebook: Differential abundance on C.virg data</li> </ul>"},{"location":"timeline/#outstanding-questions","title":"Outstanding Questions","text":"<p>Issue #39: Compare DE results from papers and create list of DEGs/markers</p> <p>Remaining to be done</p> <p>Systematic comparison with published literature still pending</p>"},{"location":"timeline/#august-2025-gsea-stepwise-approach-development","title":"August 2025: GSEA &amp; Stepwise Approach Development","text":""},{"location":"timeline/#gsea-integration","title":"GSEA Integration","text":"<p>Activity: Run differentialabundance with GSEA</p> <ul> <li>Created GMT file with gene descriptions</li> <li>Issue #45: Understand GSEA (not completed)</li> </ul> <p>Related:</p> <ul> <li>Notebook: Creating GMT file for GSEA</li> </ul>"},{"location":"timeline/#stepwise-differential-abundance","title":"Stepwise Differential Abundance","text":"<p>Issue #41: Two-step approach</p> <p>Steps:</p> <ol> <li>Step 1: Controls vs. treated (identify stress-responsive genes)</li> <li>Step 2: Resistant vs. sensitive (from step 1 genes)</li> </ol> <p>Implementation: <code>analyses/stepwise_differentialabundance/</code></p> <p>Shelly's results on dataset 1:</p> <ul> <li>Only 1 significant gene found</li> <li>Problem: DESeq isn't ideal for highly pared-down gene sets (VST couldn't work well with only ~50 genes)</li> </ul> <p>Big Lesson #3: Biomarkers in Controls</p> <p>Are we removing biomarkers that are innate? If resilience biomarkers are constitutively expressed (present in controls), the stepwise filtering approach removes them!</p>"},{"location":"timeline/#classifier-development-begins","title":"Classifier Development Begins","text":"<p>Issue #42: Validate SR320 classification results</p> <p>Question: Are the ~50 markers convincing about the difference between sensitive vs. resistant?</p> <p>Revisit: Make plots</p> <p>Need visualization to assess convincingness of candidate markers</p> <p>Issue #43: SR320's AI model</p> <p>Issue #44: Combined datasets 1 &amp; 5</p> <p>Comparison: Integrated data analysis vs. post-data integration approaches</p> <p>Result: \u2705 6-gene classifier completed!</p> <p>Pipeline:</p> <ul> <li>Step 1: Rank genes based on:</li> <li>Reproducibility across datasets</li> <li>Consistency of directionality in expression differences</li> <li>Heterogeneity assessment</li> <li>Step 2: Logistic regression to find minimum gene set for good classification</li> </ul> <p>Six-Gene Classifier Panel Identified</p> <p>Strong separation between tolerant and sensitive phenotypes achieved</p> <p>Big Lesson #4: Training Set in Test Set</p> <p>Only include training set in test set if exploring within study. If trying to predict phenotypes in other studies, you should definitely not include the training data in the test set (avoid overfitting/leakage).</p> <p>Related:</p> <ul> <li>Notebook: Two-script pipeline for gene classifier</li> <li>Notebook: Stepwise approach on dataset 1</li> </ul>"},{"location":"timeline/#september-2025-validation-characterization","title":"September 2025: Validation &amp; Characterization","text":""},{"location":"timeline/#cross-study-comparison","title":"Cross-Study Comparison","text":"<p>Issue #36: Run differentialabundance independently for each dataset and compare DEGs</p> <p>Theme: Post-data integration \u2192 do we see more overlap?</p> <p>Could return to this</p> <p>Good question about post-data integration vs. integrated data analysis, but uncertain about subsetting approach mentioned in the issue</p> <p>Status: Not completed</p>"},{"location":"timeline/#integration-attempts","title":"Integration Attempts","text":"<p>Issue #46: Integrate all data and run through differentialabundance pipeline</p> <p>Could revisit</p> <p>Another integration attempt; not completed</p> <p>Issue #47: (Not pursued; no need to revisit)</p>"},{"location":"timeline/#6-gene-panel-characterization","title":"6-Gene Panel Characterization","text":"<p>Issue #49: Plot 6 genes to gain confidence</p> <p>Goal: Visualize how well these 6 genes distinguish phenotypes</p> <p>Related:</p> <ul> <li>Notebook: Exploring six-gene biomarker across studies</li> </ul> <p>Issue #51: Replot heatmap with improved clustering and labels</p> <p>Revisit this</p> <p>Visualization improvements pending</p> <p>Issue #52: Coverage density plots</p> <p>Status: Still needs notebook entry</p>"},{"location":"timeline/#innate-vs-reactive-analysis","title":"Innate vs. Reactive Analysis","text":"<p>Issue #53: Innate vs. reactive gene expression</p> <p>Critical insight: Biomarkers may be constitutively different in resistant vs. sensitive oysters (innate), not just reactive to stress</p> <p>Related:</p> <ul> <li>Notebook: Series of DESeq2 runs indicates innate DEGs</li> </ul>"},{"location":"timeline/#future-datasets","title":"Future Datasets","text":"<p>Issue #54: Find more datasets</p> <p>Status: Postponed</p>"},{"location":"timeline/#key-takeaways","title":"Key Takeaways","text":""},{"location":"timeline/#four-big-lessons","title":"Four Big Lessons","text":"<ol> <li>Integrated analysis fails with noisy, weak-signal data \u2192 pivot to post-data integration</li> <li>Oversimplified trait definitions \u2192 study effects dominate trait effects</li> <li>Innate biomarkers exist in controls \u2192 don't filter them out in stepwise approaches</li> <li>Training/test set leakage \u2192 critical for cross-study validation</li> </ol>"},{"location":"timeline/#successful-methodologies","title":"Successful Methodologies","text":"<p>\u2705 Per-dataset differential abundance analysis \u2705 Post-data integration (compare results across datasets) \u2705 Two-step classifier pipeline (reproducibility + logistic regression) \u2705 Leave-One-Study-Out (LOSO) validation \u2705 Innate vs. reactive DEG characterization  </p>"},{"location":"timeline/#open-questions","title":"Open Questions","text":"<ul> <li>Systematic comparison with published DEG lists</li> <li>Optimal visualization of 6-gene panel across studies</li> <li>Additional dataset integration for broader validation</li> </ul> <p>Next: Explore the validated pipelines in detail: Decision Tree | Two-Step Classifier</p>"},{"location":"pipelines/classifier-path/","title":"Pipeline: Two-Step Classifier","text":""},{"location":"pipelines/classifier-path/#overview","title":"Overview","text":"<p>The two-step classifier approach identifies a minimal gene panel that distinguishes tolerant from sensitive phenotypes through:</p> <ol> <li>Step 1: Rank genes based on reproducibility, directionality consistency, and heterogeneity across datasets</li> <li>Step 2: Use logistic regression to find the minimum gene set that achieves good classification</li> </ol> <p>Status: \u2705 Validated - Produced the working 6-gene classifier panel</p>"},{"location":"pipelines/classifier-path/#when-to-use-this-approach","title":"When to Use This Approach","text":"<p>\u2705 Use when:</p> <ul> <li>You want a minimal gene panel for practical application (&lt; 10 genes)</li> <li>You have multiple independent datasets to assess reproducibility</li> <li>You need cross-study validation (LOSO)</li> <li>You don't want to exclude innate biomarkers (constitutively different)</li> </ul> <p>\u2705 Advantages over stepwise:</p> <ul> <li>Doesn't require control groups</li> <li>Preserves innate biomarkers</li> <li>Reproducibility-focused (prioritizes genes consistent across studies)</li> <li>Designed for minimal feature sets (logistic regression handles small gene counts)</li> </ul>"},{"location":"pipelines/classifier-path/#the-pipeline","title":"The Pipeline","text":""},{"location":"pipelines/classifier-path/#prerequisites","title":"Prerequisites","text":"<ul> <li>Multiple RNA-seq datasets with comparable phenotype labels (tolerant/sensitive)</li> <li>Gene count matrices for each dataset</li> <li>Metadata with phenotype labels</li> <li>Shared gene annotation across datasets</li> </ul>"},{"location":"pipelines/classifier-path/#input-data-structure","title":"Input Data Structure","text":"<p>This approach uses post-data integration:</p> <ul> <li>Analyze each dataset independently first</li> <li>Compare results across datasets</li> <li>Identify reproducible signals</li> </ul> <pre><code>Dataset 1: [counts, metadata with phenotype]\nDataset 2: [counts, metadata with phenotype]\nDataset 3: [counts, metadata with phenotype]\n...\n</code></pre> <p>Important: Do NOT pool datasets before analysis (see Problem Framing - Big Lesson #1)</p>"},{"location":"pipelines/classifier-path/#step-1-reproducibility-based-gene-ranking","title":"Step 1: Reproducibility-Based Gene Ranking","text":""},{"location":"pipelines/classifier-path/#goal","title":"Goal","text":"<p>Identify genes that consistently differentiate phenotypes across multiple studies</p>"},{"location":"pipelines/classifier-path/#criteria","title":"Criteria","text":"<p>For each gene, assess:</p> <ol> <li>Reproducibility: Gene is differentially expressed in multiple datasets</li> <li>Directionality consistency: Direction of fold change is same across datasets (e.g., always upregulated in tolerant)</li> <li>Heterogeneity: Variance in expression within phenotype groups</li> </ol>"},{"location":"pipelines/classifier-path/#implementation-approach","title":"Implementation Approach","text":"<pre><code># Pseudocode for Step 1\n\nfor each_dataset in datasets:\n    # Run differential expression\n    deg_results[dataset] = DESeq2(\n        counts=dataset.counts,\n        design=\"~ phenotype\"\n    )\n\n# Score genes across datasets\ngene_scores = {}\nfor gene in all_genes:\n    # Count how many datasets show significant DE\n    reproducibility = count_significant_across_datasets(gene, deg_results)\n\n    # Check directionality consistency\n    fold_changes = [deg_results[ds][gene].log2FC for ds in datasets]\n    directionality = 1.0 if all_same_sign(fold_changes) else 0.0\n\n    # Assess heterogeneity\n    heterogeneity = calculate_within_group_variance(gene, datasets)\n\n    # Combined score\n    gene_scores[gene] = (\n        reproducibility * directionality / heterogeneity\n    )\n\n# Rank genes by score\nranked_genes = sort_descending(gene_scores)\n</code></pre>"},{"location":"pipelines/classifier-path/#output","title":"Output","text":"<p>Ranked list of candidate genes, prioritizing those that:</p> <ul> <li>Appear as DEGs in multiple datasets</li> <li>Show consistent direction of change</li> <li>Have low within-group variance (high between-group separation)</li> </ul>"},{"location":"pipelines/classifier-path/#step-2-logistic-regression-for-minimal-panel","title":"Step 2: Logistic Regression for Minimal Panel","text":""},{"location":"pipelines/classifier-path/#goal_1","title":"Goal","text":"<p>Find the smallest set of genes that achieves good classification accuracy</p>"},{"location":"pipelines/classifier-path/#method","title":"Method","text":"<p>Use logistic regression with regularization (LASSO or elastic net) to select features:</p> <pre><code># Pseudocode for Step 2\n\n# Start with top N genes from Step 1 (e.g., top 50-100)\ncandidate_genes = ranked_genes[:100]\n\n# Prepare training data (combined datasets)\nX_train = combined_expression_matrix[candidate_genes]\ny_train = phenotype_labels  # tolerant vs. sensitive\n\n# Fit logistic regression with LASSO regularization\nfrom sklearn.linear_model import LogisticRegressionCV\n\nclf = LogisticRegressionCV(\n    penalty='l1',  # LASSO for feature selection\n    solver='liblinear',\n    cv=5,  # cross-validation to find optimal regularization\n    max_iter=1000\n)\nclf.fit(X_train, y_train)\n\n# Extract selected genes (non-zero coefficients)\nselected_genes = candidate_genes[clf.coef_[0] != 0]\n\nprint(f\"Minimal gene panel: {len(selected_genes)} genes\")\nprint(selected_genes)\n</code></pre>"},{"location":"pipelines/classifier-path/#regularization-parameter-selection","title":"Regularization Parameter Selection","text":"<ul> <li>Use cross-validation to find optimal L1 penalty</li> <li>Balance between:</li> <li>Fewer genes (higher penalty) \u2192 simpler assay, may sacrifice accuracy</li> <li>More genes (lower penalty) \u2192 better accuracy, more complex assay</li> </ul>"},{"location":"pipelines/classifier-path/#implementation-example-6-gene-classifier","title":"Implementation Example: 6-Gene Classifier","text":"<p>From Issue #44 and notebook post:</p>"},{"location":"pipelines/classifier-path/#datasets-used","title":"Datasets Used","text":"<ul> <li>Dataset 1: C. virginica with P. marinus exposure</li> <li>Dataset 5: C. virginica with P. marinus exposure</li> </ul>"},{"location":"pipelines/classifier-path/#results","title":"Results","text":"<p>Step 1 output: Ranked ~1000s of genes by reproducibility</p> <p>Step 2 output: 6-gene panel with strong phenotype separation</p>"},{"location":"pipelines/classifier-path/#performance","title":"Performance","text":"<p>\u2705 Strong separation between tolerant and sensitive phenotypes \u2705 Genes are reproducible across independent datasets \u2705 Small enough for practical assay development  </p> <p>Related notebook: Two-script pipeline for gene classifier</p>"},{"location":"pipelines/classifier-path/#validation-leave-one-study-out-loso","title":"Validation: Leave-One-Study-Out (LOSO)","text":"<p>Big Lesson #4: Avoid Training Set in Test Set</p> <p>When evaluating cross-study performance:</p> <ul> <li>Within-study validation: OK to include training samples if only evaluating performance within same study</li> <li>Cross-study validation: Never include training study in test set \u2192 use LOSO</li> </ul> <p>Including training data in test set leads to overfitting and falsely optimistic accuracy.</p>"},{"location":"pipelines/classifier-path/#loso-procedure","title":"LOSO Procedure","text":"<p>For each dataset:</p> <ol> <li>Train classifier on all OTHER datasets</li> <li>Test on held-out dataset (never seen during training)</li> <li>Report accuracy, sensitivity, specificity</li> </ol> <pre><code># LOSO Cross-Validation\nfor test_dataset in all_datasets:\n    training_datasets = [ds for ds in all_datasets if ds != test_dataset]\n\n    # Train on all except test_dataset\n    X_train = combine_datasets(training_datasets, selected_genes)\n    y_train = get_labels(training_datasets)\n    clf.fit(X_train, y_train)\n\n    # Test on held-out dataset\n    X_test = test_dataset[selected_genes]\n    y_test = test_dataset['phenotype']\n    accuracy = clf.score(X_test, y_test)\n\n    print(f\"Test on {test_dataset.name}: Accuracy = {accuracy:.3f}\")\n</code></pre>"},{"location":"pipelines/classifier-path/#expected-performance","title":"Expected Performance","text":"<ul> <li>High accuracy (&gt; 80%) across LOSO folds suggests robust, generalizable panel</li> <li>Variable accuracy suggests study-specific effects; may need batch correction or dataset filtering</li> </ul> <p>See Validation &amp; Pitfalls for more on LOSO and other validation strategies.</p>"},{"location":"pipelines/classifier-path/#innate-vs-reactive-biomarkers","title":"Innate vs. Reactive Biomarkers","text":"<p>A key advantage of this approach: it captures innate biomarkers</p> <p>Innate Biomarker Preservation</p> <p>Unlike the stepwise approach, the classifier method does not filter based on stress response.</p> <p>Genes that are constitutively different between resistant/sensitive oysters (even in controls) will be: - Identified in Step 1 if reproducible across datasets - Selected in Step 2 if predictive of phenotype</p> <p>Analysis: Issue #53 and notebook</p> <p>To characterize whether your biomarkers are innate or reactive:</p> <ul> <li>Compare gene expression in controls only (resistant vs. sensitive)</li> <li>If significant \u2192 innate biomarker</li> <li>If not significant in controls but significant in treated \u2192 reactive biomarker</li> </ul>"},{"location":"pipelines/classifier-path/#workflow-summary","title":"Workflow Summary","text":"<pre><code>graph TD\n    A[Multiple Datasets&lt;br/&gt;Independent DE Analysis] --&gt; B[Step 1: Gene Scoring]\n    B --&gt; C[Reproducibility:&lt;br/&gt;Present in multiple datasets]\n    B --&gt; D[Directionality:&lt;br/&gt;Consistent fold change sign]\n    B --&gt; E[Heterogeneity:&lt;br/&gt;Low within-group variance]\n    C --&gt; F[Ranked Gene List]\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G[Step 2: Logistic Regression&lt;br/&gt;with LASSO]\n    G --&gt; H[Minimal Gene Panel&lt;br/&gt;e.g., 6 genes]\n    H --&gt; I[LOSO Validation]\n    I --&gt; J{Accuracy &gt; 80%&lt;br/&gt;across folds?}\n    J --&gt;|Yes| K[Validated Classifier]\n    J --&gt;|No| L[Refine: Add features,&lt;br/&gt;adjust penalty, filter datasets]\n</code></pre>"},{"location":"pipelines/classifier-path/#code-resources","title":"Code &amp; Resources","text":"<p>Primary analysis repository: <code>Cvirg_Pmarinus_RNAseq/analyses/</code></p> <p>Key notebook: Two-script pipeline for gene-expression classifier</p> <p>Related issues:</p> <ul> <li>Issue #44: Combined datasets 1 &amp; 5</li> <li>Issue #49: Plot 6 genes</li> <li>Issue #53: Innate vs. reactive</li> </ul> <p>Additional exploration:</p> <ul> <li>Exploring six-gene biomarker across studies</li> <li>Common genes per LOSO fold</li> </ul>"},{"location":"pipelines/classifier-path/#practical-considerations","title":"Practical Considerations","text":""},{"location":"pipelines/classifier-path/#minimum-dataset-requirements","title":"Minimum Dataset Requirements","text":"<ul> <li>At least 2 independent studies (for reproducibility assessment)</li> <li>Comparable phenotype definitions (resistant/sensitive)</li> <li>Sufficient sample size (n &gt; 10 per group per study recommended)</li> </ul>"},{"location":"pipelines/classifier-path/#computational-requirements","title":"Computational Requirements","text":"<ul> <li>Moderate: Can run on laptop/workstation</li> <li>No need for HPC (unlike full nf-core pipelines)</li> <li>Python/R with standard ML libraries (scikit-learn, glmnet)</li> </ul>"},{"location":"pipelines/classifier-path/#assay-development","title":"Assay Development","text":"<p>Once you have a minimal panel:</p> <ul> <li>6-10 genes \u2192 feasible for qPCR assay</li> <li>&lt; 20 genes \u2192 feasible for targeted RNA-seq panel</li> <li>&gt; 20 genes \u2192 may need full RNA-seq (cost-prohibitive for routine screening)</li> </ul> <p>Next: Learn about validation strategies and common pitfalls in Validation &amp; Pitfalls</p>"},{"location":"pipelines/decision-tree/","title":"Pipeline: Stepwise Differential Abundance","text":""},{"location":"pipelines/decision-tree/#overview","title":"Overview","text":"<p>The stepwise differential abundance approach attempts to identify resilience biomarkers by filtering genes in two sequential steps:</p> <ol> <li>Step 1: Identify stress-responsive genes (control vs. treated)</li> <li>Step 2: Identify resistance-associated genes (from Step 1 genes, compare resistant vs. sensitive)</li> </ol> <p>Status: \u26a0\ufe0f Partially validated - works in some contexts but has critical limitations</p>"},{"location":"pipelines/decision-tree/#when-to-use-this-approach","title":"When to Use This Approach","text":"<p>\u2705 Use when:</p> <ul> <li>You want to identify genes that are both stress-responsive AND differentiate phenotypes</li> <li>You have clear control and treatment groups within studies</li> <li>Your biological hypothesis is that biomarkers are reactive (induced by stress)</li> </ul> <p>\u274c Don't use when:</p> <ul> <li>You suspect innate biomarkers (constitutively different between resistant/sensitive)</li> <li>You have very small gene sets (&lt; 100 genes) after filtering</li> <li>Study design doesn't include both controls and treated samples</li> </ul>"},{"location":"pipelines/decision-tree/#the-pipeline","title":"The Pipeline","text":""},{"location":"pipelines/decision-tree/#prerequisites","title":"Prerequisites","text":"<ul> <li>Per-dataset gene count matrices</li> <li>Metadata with phenotype and treatment labels</li> <li>Reference genome/transcriptome annotation</li> </ul>"},{"location":"pipelines/decision-tree/#step-1-identify-stress-responsive-genes","title":"Step 1: Identify Stress-Responsive Genes","text":"<p>Goal: Find genes differentially expressed between control and treated samples (regardless of phenotype)</p> <p>Method: Use DESeq2 or similar differential expression tool</p> <pre><code># Pseudocode example\ndds &lt;- DESeqDataSetFromMatrix(\n  countData = counts,\n  colData = metadata,\n  design = ~ treatment  # control vs. treated\n)\ndds &lt;- DESeq(dds)\nresults_step1 &lt;- results(dds, alpha = 0.05)\n\n# Filter for significant genes\nstress_responsive &lt;- results_step1[results_step1$padj &lt; 0.05, ]\n</code></pre> <p>Output: List of stress-responsive genes (typically 50-5000 genes depending on study and threshold)</p>"},{"location":"pipelines/decision-tree/#step-2-identify-resistance-associated-genes","title":"Step 2: Identify Resistance-Associated Genes","text":"<p>Goal: From Step 1 genes, find those that differentiate resistant from sensitive phenotypes</p> <p>Method: Subset counts to Step 1 genes, then run differential expression between phenotypes</p> <pre><code># Subset to stress-responsive genes only\ncounts_filtered &lt;- counts[rownames(counts) %in% rownames(stress_responsive), ]\n\n# Compare resistant vs. sensitive on filtered genes\ndds2 &lt;- DESeqDataSetFromMatrix(\n  countData = counts_filtered,\n  colData = metadata,\n  design = ~ phenotype  # resistant vs. sensitive\n)\ndds2 &lt;- DESeq(dds2)\nresults_step2 &lt;- results(dds2, alpha = 0.05)\n</code></pre> <p>Output: Candidate resilience biomarkers</p>"},{"location":"pipelines/decision-tree/#implementation-example-dataset-1","title":"Implementation Example: Dataset 1","text":"<p>From Issue #41 and notebook post:</p> <p>Results:</p> <ul> <li>Step 1 produced ~50 stress-responsive genes</li> <li>Step 2 identified only 1 significant gene</li> </ul> <p>Analysis in: <code>analyses/stepwise_differentialabundance/</code></p>"},{"location":"pipelines/decision-tree/#critical-limitations","title":"Critical Limitations","text":""},{"location":"pipelines/decision-tree/#1-deseq2-breaks-down-with-small-gene-sets","title":"1. DESeq2 Breaks Down with Small Gene Sets","text":"<p>Small Gene Set Problem</p> <p>When Step 1 produces &lt; 100 genes, DESeq2's variance stabilizing transformation (VST) becomes unreliable. With only ~50 genes:</p> <ul> <li>Not enough data for robust dispersion estimation</li> <li>VST assumes thousands of genes for stable transformation</li> <li>Results may be spurious</li> </ul> <p>Observed: DESeq2 on 50 genes produced weak/unconvincing results</p>"},{"location":"pipelines/decision-tree/#2-removes-innate-biomarkers","title":"2. Removes Innate Biomarkers","text":"<p>Big Lesson #3: Filtering Out Innate Signals</p> <p>Biomarkers may exist in control groups if they represent innate resilience (genes constitutively different in resistant vs. sensitive individuals, even before stress).</p> <p>The stepwise approach removes these by design because Step 1 filters for differential expression between control/treated.</p> <p>If a gene is expressed at different baseline levels in resistant vs. sensitive oysters but doesn't change with stress, it will be filtered out in Step 1.</p> <p>See Issue #53 and innate gene expression analysis</p>"},{"location":"pipelines/decision-tree/#3-study-specific-effects-still-dominate","title":"3. Study-Specific Effects Still Dominate","text":"<p>Even with stepwise filtering:</p> <ul> <li>Batch effects persist</li> <li>Per-dataset analysis required (not cross-study integration)</li> <li>Results may not generalize to other studies</li> </ul>"},{"location":"pipelines/decision-tree/#decision-tree-should-you-use-this-pipeline","title":"Decision Tree: Should You Use This Pipeline?","text":"<pre><code>graph TD\n    A[Start: Biomarker Discovery Goal] --&gt; B{Do you have control&lt;br/&gt;and treated groups?}\n    B --&gt;|No| C[Use classifier approach]\n    B --&gt;|Yes| D{Do you believe biomarkers&lt;br/&gt;are reactive to stress?}\n    D --&gt;|No / Unsure| C\n    D --&gt;|Yes| E{Will Step 1 yield&lt;br/&gt;&gt; 100 genes?}\n    E --&gt;|Unsure / No| C\n    E --&gt;|Yes| F[Use stepwise approach]\n    F --&gt; G{Does Step 2 yield&lt;br/&gt;convincing results?}\n    G --&gt;|No| C\n    G --&gt;|Yes| H[Validate with LOSO]\n    C --&gt; I[Two-Step Classifier Pipeline]\n</code></pre>"},{"location":"pipelines/decision-tree/#alternative-two-step-classifier","title":"Alternative: Two-Step Classifier","text":"<p>If the stepwise approach doesn't work for your data, consider the Two-Step Classifier, which:</p> <ul> <li>Doesn't filter out innate biomarkers</li> <li>Handles any dataset design (doesn't require controls)</li> <li>Uses reproducibility scoring instead of sequential filtering</li> <li>Produced the validated 6-gene panel in this project</li> </ul>"},{"location":"pipelines/decision-tree/#validation-requirements","title":"Validation Requirements","text":"<p>If you proceed with stepwise approach:</p> <ol> <li>Within-study validation: Can the genes predict phenotypes in held-out samples from the same study?</li> <li>Cross-study validation: Use Leave-One-Study-Out (LOSO) - see Validation &amp; Pitfalls</li> <li>Biological validation: Are genes functionally related to stress response pathways?</li> </ol>"},{"location":"pipelines/decision-tree/#related-resources","title":"Related Resources","text":"<ul> <li>Issue #41: Stepwise approach</li> <li>Notebook: Stepwise on dataset 1</li> <li>Issue #53: Innate vs. reactive</li> <li>nf-core/differentialabundance docs</li> </ul> <p>Next: See the more successful Two-Step Classifier approach, or learn about Validation &amp; Pitfalls</p>"},{"location":"pipelines/validation/","title":"Validation &amp; Pitfalls","text":""},{"location":"pipelines/validation/#overview","title":"Overview","text":"<p>Validation is critical for ensuring biomarker panels generalize beyond the discovery cohort. This guide covers validation strategies and common pitfalls encountered in this project, with emphasis on what actually went wrong and how to avoid it.</p>"},{"location":"pipelines/validation/#validation-strategies","title":"Validation Strategies","text":""},{"location":"pipelines/validation/#1-leave-one-study-out-loso-cross-validation","title":"1. Leave-One-Study-Out (LOSO) Cross-Validation","text":"<p>What it is: Train classifier on all datasets except one, test on the held-out dataset</p> <p>Why it's essential:</p> <ul> <li>Tests cross-study generalization</li> <li>Avoids overfitting to study-specific effects</li> <li>Simulates real-world scenario: predict phenotypes in new, unseen studies</li> </ul> <p>Implementation:</p> <pre><code>datasets = [dataset1, dataset2, dataset3, dataset4, dataset5]\n\nfor test_idx, test_dataset in enumerate(datasets):\n    # Training set: all except test_dataset\n    train_datasets = [ds for i, ds in enumerate(datasets) if i != test_idx]\n\n    X_train = combine_expression_data(train_datasets, gene_panel)\n    y_train = combine_labels(train_datasets)\n\n    # Fit classifier\n    classifier.fit(X_train, y_train)\n\n    # Test on held-out study\n    X_test = test_dataset.expression[gene_panel]\n    y_test = test_dataset.labels\n\n    accuracy = classifier.score(X_test, y_test)\n    print(f\"LOSO Fold {test_idx+1} (test={test_dataset.name}): {accuracy:.3f}\")\n</code></pre> <p>Interpretation:</p> <ul> <li>\u2705 Good: Consistent accuracy (e.g., 0.80-0.90) across all folds</li> <li>\u26a0\ufe0f Warning: One fold significantly lower \u2192 investigate study-specific effects</li> <li>\u274c Poor: High variance in accuracy or &lt; 0.70 average \u2192 panel doesn't generalize</li> </ul> <p>Big Lesson #4: Training Set in Test Set</p> <p>Never include the training study in the test set when evaluating cross-study performance. This leads to data leakage and falsely optimistic results.</p> <p>\u2705 Correct: LOSO (train on studies 1,2,3; test on study 4) \u274c Wrong: Train on studies 1,2,3,4; test on studies 1,2,3,4</p> <p>Related: Issue #44</p>"},{"location":"pipelines/validation/#2-within-study-cross-validation","title":"2. Within-Study Cross-Validation","text":"<p>What it is: Hold out samples within a single study (k-fold or train/test split)</p> <p>When to use:</p> <ul> <li>Initial model development and parameter tuning</li> <li>Assessing overfitting within a study</li> <li>When you only have one study available (but be cautious about generalizability)</li> </ul> <p>Limitations:</p> <ul> <li>Does NOT test cross-study generalization</li> <li>May overestimate performance if study-specific effects are strong</li> <li>Use as a first step, but follow with LOSO</li> </ul>"},{"location":"pipelines/validation/#3-independent-validation-cohort","title":"3. Independent Validation Cohort","text":"<p>Gold standard: Test on completely independent dataset never used during development</p> <p>Challenges:</p> <ul> <li>Requires finding additional suitable datasets</li> <li>May have different experimental designs or phenotype definitions</li> <li>Labor-intensive to process and integrate</li> </ul> <p>Status in this project: Issue #54 (find more datasets) - postponed</p>"},{"location":"pipelines/validation/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"pipelines/validation/#1-data-leakage","title":"1. Data Leakage","text":"<p>Definition: Information from test set \"leaks\" into training process, causing overly optimistic performance estimates</p>"},{"location":"pipelines/validation/#leakage-scenario-1-normalization-across-trainingtest","title":"Leakage Scenario 1: Normalization Across Training+Test","text":"<p>\u274c Wrong:</p> <pre><code># Normalize all data together\nall_data_normalized = vst_transform(combine(train_data, test_data))\n\n# Then split\nX_train = all_data_normalized[train_indices]\nX_test = all_data_normalized[test_indices]  # LEAKAGE!\n</code></pre> <p>\u2705 Correct:</p> <pre><code># Fit normalization on training data only\nvst_params = fit_vst(train_data)\n\n# Apply to train and test separately\nX_train = apply_vst(train_data, vst_params)\nX_test = apply_vst(test_data, vst_params)\n</code></pre>"},{"location":"pipelines/validation/#leakage-scenario-2-feature-selection-on-all-data","title":"Leakage Scenario 2: Feature Selection on All Data","text":"<p>\u274c Wrong:</p> <pre><code># Select features using all data\nselected_genes = select_top_degs(all_data, phenotypes)  # LEAKAGE!\n\n# Then split for training/testing\nX_train = train_data[selected_genes]\nX_test = test_data[selected_genes]\n</code></pre> <p>\u2705 Correct:</p> <pre><code># Select features using training data only\nselected_genes = select_top_degs(train_data, train_phenotypes)\n\n# Apply to test\nX_train = train_data[selected_genes]\nX_test = test_data[selected_genes]\n</code></pre> <p>In this project: LOSO properly avoids leakage by keeping test studies completely separate during training</p>"},{"location":"pipelines/validation/#2-batch-effects","title":"2. Batch Effects","text":"<p>Definition: Non-biological variation between studies due to technical differences (sequencing platform, library prep, time, lab)</p> <p>Big Lessons #1 &amp; #2: When Batch Effects Dominate</p> <ul> <li>Study-specific effects were consistently stronger than trait effects</li> <li>Attempted corrections (COMBAT, RemoveBatchEffect) had little effect on trait separation</li> <li>Pivot: Abandoned integrated analysis in favor of post-data integration</li> </ul> <p>Observations:</p> <ul> <li>PCA plots showed clustering by study, not by phenotype</li> <li>Trait-based separation was weak even after batch correction</li> <li>Integration worked only when combining very similar studies (e.g., study 4 injected + study 5)</li> </ul>"},{"location":"pipelines/validation/#batch-correction-attempts","title":"Batch Correction Attempts","text":"<p>Tried:</p> <ul> <li>Issue #18: COMBAT and RemoveBatchEffect</li> </ul> <p>Outcome: \u274c Insufficient improvement</p> <p>Lesson: Batch correction is not a panacea. When study effects are too strong, no correction method will recover trait signal.</p>"},{"location":"pipelines/validation/#post-data-integration-solution","title":"Post-Data Integration Solution","text":"<p>Instead of correcting batches and pooling:</p> <ol> <li>Analyze each study independently</li> <li>Identify genes significant in multiple studies</li> <li>Build classifier on reproducible genes</li> </ol> <p>Advantage: Reproducibility across studies implicitly handles batch effects</p> <p>Related: Timeline - February 2025</p>"},{"location":"pipelines/validation/#3-normalization-timing-method","title":"3. Normalization Timing &amp; Method","text":""},{"location":"pipelines/validation/#understanding-the-pipeline","title":"Understanding the Pipeline","text":"<p>In <code>nf-core/differentialabundance</code>:</p> <ol> <li>Raw counts are input</li> <li>PCAs are generated before differential abundance analysis (on raw or lightly filtered counts)</li> <li>Normalization (VST, rlog, TPM) happens during DESeq2 analysis</li> <li>Batch correction (if enabled) is applied post-normalization</li> </ol> <p>Normalization Insight from July 2025</p> <p>Understanding when normalization happens is critical for interpreting QC plots:</p> <ul> <li>Initial PCAs show raw/lightly-processed data</li> <li>Don't expect trait separation in initial PCAs if normalization hasn't been applied</li> <li>Post-analysis PCAs on normalized data are more informative</li> </ul> <p>Related: Issue #34, Timeline - July 2025</p>"},{"location":"pipelines/validation/#normalization-methods","title":"Normalization Methods","text":"<p>VST (Variance-Stabilizing Transformation):</p> <ul> <li>Recommended for visualization and clustering</li> <li>Requires sufficient genes (&gt; 1000) for stable estimation</li> <li>Breaks down with small gene sets (&lt; 100)</li> </ul> <p>VST with Small Gene Sets</p> <p>In the stepwise approach, Step 1 filtering left only ~50 genes. DESeq2's VST could not work properly with such a small set.</p> <p>Symptom: Unreliable dispersion estimates, poor model fit Solution: Use alternative normalization (TMM, TPM) or abandon stepwise approach</p> <p>Related: Stepwise Pipeline</p>"},{"location":"pipelines/validation/#4-oversimplified-phenotype-definitions","title":"4. Oversimplified Phenotype Definitions","text":"<p>Big Lesson #2: Traits Were Oversimplified</p> <p>Initial approach used generalized trait labels across studies:</p> <ul> <li>\"Stress\" vs. \"Control\"</li> <li>\"Resistant\" vs. \"Sensitive\"</li> </ul> <p>Problem: Different studies measured different stressors with different designs:</p> <ul> <li>Study 1: Disease challenge, time point A</li> <li>Study 2: Temperature stress, time point B</li> <li>Study 3: Disease challenge, time point C, different infection route</li> </ul> <p>Result: Trait effects were much weaker than study-specific effects</p> <p>Solution: Use more specific, harmonized phenotype definitions</p> <ul> <li>Compare only studies with similar experimental designs</li> <li>Within-study trait comparisons first, then assess reproducibility across studies</li> <li>Document and respect phenotype heterogeneity</li> </ul> <p>Related: Problem Framing - Defining the Phenotype</p>"},{"location":"pipelines/validation/#5-small-sample-sizes","title":"5. Small Sample Sizes","text":"<p>Challenges:</p> <ul> <li>Insufficient power to detect true DEGs</li> <li>High variance in estimates</li> <li>Overfitting risk (model learns noise, not signal)</li> </ul> <p>Mitigations:</p> <ul> <li>Use regularized models (LASSO, elastic net) to reduce overfitting</li> <li>Focus on genes reproducible across studies (increases effective n)</li> <li>Report confidence intervals, not just point estimates</li> </ul> <p>In this project: Combined datasets (e.g., Issue #34 merged study 4 + 5) to increase sample size</p>"},{"location":"pipelines/validation/#6-innate-vs-reactive-biomarkers","title":"6. Innate vs. Reactive Biomarkers","text":"<p>Big Lesson #3: Biomarkers in Controls</p> <p>Biomarkers may be constitutively expressed (innate) in resistant vs. sensitive individuals, even in the absence of stress.</p> <p>Implication: If you filter based on \"stress-responsive\" (control vs. treated), you will remove innate biomarkers.</p> <p>Detection:</p> <ul> <li>Compare resistant vs. sensitive in control samples only</li> <li>If significant \u2192 innate biomarker</li> <li>If not significant in controls but significant in treated \u2192 reactive biomarker</li> </ul> <p>Pipeline choice:</p> <ul> <li>\u2705 Two-step classifier preserves innate biomarkers</li> <li>\u274c Stepwise approach removes innate biomarkers by design</li> </ul> <p>Related:</p> <ul> <li>Issue #53: Innate vs. reactive analysis</li> <li>Notebook: Innate gene expression</li> </ul>"},{"location":"pipelines/validation/#validation-checklist","title":"Validation Checklist","text":"<p>Before claiming a validated biomarker panel:</p> <ul> <li>[ ] Cross-study validation: LOSO accuracy &gt; 70% (ideally &gt; 80%)</li> <li>[ ] No data leakage: Normalization, feature selection, and hyperparameter tuning done independently per fold</li> <li>[ ] Batch effect assessment: Confirmed genes are reproducible across studies, not driven by batch</li> <li>[ ] Sample size adequacy: Each phenotype group has n &gt; 10 per study (if possible)</li> <li>[ ] Phenotype definition: Clear, specific, and documented; comparable across studies used</li> <li>[ ] Innate/reactive characterization: Assessed whether biomarkers are pre-existing or induced</li> <li>[ ] Biological validation: Genes have known or plausible functional roles in stress response</li> </ul>"},{"location":"pipelines/validation/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"pipelines/validation/#poor-loso-performance","title":"Poor LOSO Performance","text":"<p>Symptom: Accuracy &lt; 0.70 or high variance across folds</p> <p>Possible causes:</p> <ol> <li>Study-specific effects too strong \u2192 Filter datasets or use more stringent reproducibility criteria in Step 1</li> <li>Phenotype heterogeneity \u2192 Ensure comparable phenotype definitions</li> <li>Small sample sizes \u2192 Combine compatible studies or seek additional data</li> <li>Overfitting \u2192 Increase regularization penalty or reduce feature set</li> </ol>"},{"location":"pipelines/validation/#genes-dont-validate","title":"Genes Don't Validate","text":"<p>Symptom: DEGs from discovery cohort not significant in validation cohort</p> <p>Possible causes:</p> <ol> <li>Batch effects \u2192 Apply batch-aware normalization or use post-data integration</li> <li>False discoveries \u2192 Discovery set had high FDR; use more stringent thresholds</li> <li>Different experimental conditions \u2192 Validation study not truly comparable</li> <li>Underpowered validation \u2192 Validation cohort too small to detect effects</li> </ol>"},{"location":"pipelines/validation/#suspect-data-leakage","title":"Suspect Data Leakage","text":"<p>Symptom: Training accuracy much lower than test accuracy, or perfect test accuracy</p> <p>Diagnostic steps:</p> <ol> <li>Re-run analysis with explicit train/test separation at each step</li> <li>Check if normalization used all data before splitting</li> <li>Verify feature selection used only training data</li> <li>Ensure no test samples used in hyperparameter tuning</li> </ol>"},{"location":"pipelines/validation/#related-resources","title":"Related Resources","text":"<p>Key issues:</p> <ul> <li>#18: Batch correction attempts</li> <li>#34: Combining studies</li> <li>#44: Classifier development with LOSO</li> <li>#53: Innate vs. reactive</li> </ul> <p>Key notebooks:</p> <ul> <li>Gene classifier panel (LOSO validation)</li> <li>Innate gene expression analysis</li> <li>Six-gene biomarker exploration</li> </ul> <p>Next: Explore detailed project history in the Timeline or browse Source Indices</p>"},{"location":"sources/issues-index/","title":"GitHub Issues Index","text":"<p>This index organizes the GitHub issues from the Cvirg_Pmarinus_RNAseq repository that were referenced in the project timeline and field guide. Issues are grouped thematically for easier navigation.</p> <p>All issue links point to: <code>https://github.com/Resilience-Biomarkers-for-Aquaculture/Cvirg_Pmarinus_RNAseq/issues/[number]</code></p>"},{"location":"sources/issues-index/#data-preparation-integration","title":"Data Preparation &amp; Integration","text":""},{"location":"sources/issues-index/#issue-3-created-merged-metadata","title":"Issue #3: Created Merged Metadata","text":"<p>Timeline: January 2025 Status: Completed Summary: Initial effort to create unified metadata across multiple RNA-seq datasets for integrated analysis.</p>"},{"location":"sources/issues-index/#issue-9-add-study-5","title":"Issue #9: Add Study 5","text":"<p>Timeline: January 2025 Status: Completed Summary: Incorporated an additional dataset (Study 5) into the analysis pipeline to increase sample size and assess reproducibility.</p>"},{"location":"sources/issues-index/#issue-54-find-more-datasets","title":"Issue #54: Find More Datasets","text":"<p>Timeline: September 2025 Status: Postponed Summary: Exploration of additional RNA-seq datasets for broader validation. Deferred to focus on existing dataset analysis.</p>"},{"location":"sources/issues-index/#differential-abundance-analysis","title":"Differential Abundance Analysis","text":""},{"location":"sources/issues-index/#issue-4-differential-abundance-initial-approach","title":"Issue #4: Differential Abundance Initial Approach","text":"<p>Timeline: January 2025 Status: Completed Summary: First attempt at differential abundance analysis. Toy example succeeded, but encountered GTF file issues and weak trait separation in full datasets.</p> <p>Key finding: Mutual information analysis on Perkinsus datasets showed insufficient separation between tolerant/sensitive groups.</p>"},{"location":"sources/issues-index/#issue-12-decide-best-methods-and-execute","title":"Issue #12: Decide Best Methods and Execute","text":"<p>Timeline: January 2025 Status: Completed Summary: Decision point for selecting optimal differential abundance methodologies after initial exploration revealed data integration challenges.</p>"},{"location":"sources/issues-index/#issue-29-differential-abundance-on-all-datasets-together","title":"Issue #29: Differential Abundance on All Datasets Together","text":"<p>Timeline: April 2025 Status: Deferred Summary: Ran differential abundance on all datasets together but results were not yet interpreted. Could return to this analysis.</p>"},{"location":"sources/issues-index/#issue-31-interpret-combined-dataset-results","title":"Issue #31: Interpret Combined Dataset Results","text":"<p>Timeline: April 2025 Status: Deferred Related: Analysis results Summary: Follow-up to #29; interpretation was postponed to focus on per-dataset approaches.</p>"},{"location":"sources/issues-index/#issue-32-run-differential-abundance-on-datasets-separately","title":"Issue #32: Run Differential Abundance on Datasets Separately","text":"<p>Timeline: June 2025 Status: Completed Summary: Pivot to per-dataset analysis. Steve focused on Study 5, Shelly focused on Study 1. Goal was understanding optimal parameters for differential abundance pipeline.</p>"},{"location":"sources/issues-index/#issue-36-compare-degs-across-datasets","title":"Issue #36: Compare DEGs Across Datasets","text":"<p>Timeline: September 2025 Status: Not completed Summary: Compare differential abundance results run independently for each dataset. Theme: post-data integration approach. Question remains about reproducibility vs. integrated data analysis, but subsetting approach is uncertain.</p>"},{"location":"sources/issues-index/#issue-46-integrate-all-data-through-differential-abundance-pipeline","title":"Issue #46: Integrate All Data Through Differential Abundance Pipeline","text":"<p>Timeline: September 2025 Status: Not completed Summary: Another attempt at integrated data analysis. Could revisit.</p>"},{"location":"sources/issues-index/#batch-effects-normalization","title":"Batch Effects &amp; Normalization","text":""},{"location":"sources/issues-index/#issue-18-batch-correction-attempts","title":"Issue #18: Batch Correction Attempts","text":"<p>Timeline: February 2025 Status: Completed Summary: Attempted batch correction using COMBAT and RemoveBatchEffect methods. Results showed little improvement in trait-based separation.</p> <p>Key lesson: Study-specific effects were stronger than trait effects; batch correction couldn't recover sufficient signal.</p>"},{"location":"sources/issues-index/#issue-34-combine-study-4-injected-study-5","title":"Issue #34: Combine Study 4 Injected + Study 5","text":"<p>Timeline: July 2025 Status: Completed Summary: Experimental combination of compatible studies (Study 4 injected group + Study 5) to increase sample size.</p> <p>Research question: Would Study 4 injected samples cluster with resistant or susceptible phenotype from Study 5?</p> <p>Learning: Gained understanding of normalization timing in differentialabundance pipeline (PCAs before analysis, normalization during). Started seeing evidence of innate trait.</p> <p>Could return to: Revisit analysis on 567 significant DEGs (by DESeq) to see if clustering improves compared to top 500 most variable genes.</p>"},{"location":"sources/issues-index/#technical-issues-parameters","title":"Technical Issues &amp; Parameters","text":""},{"location":"sources/issues-index/#issue-26-parameter-selection-gc-bias","title":"Issue #26: Parameter Selection &amp; GC Bias","text":"<p>Timeline: April 2025 Status: Completed Summary: Identified that Johnson dataset used TAG-seq (not standard RNA-seq) and discovered GC bias. Determined that initial analysis parameters were inappropriate for TAG-seq data.</p>"},{"location":"sources/issues-index/#issue-28-rerun-johnson-data-with-different-parameters","title":"Issue #28: Rerun Johnson Data with Different Parameters","text":"<p>Timeline: April 2025 Status: Completed Related: Notebook post Summary: Reprocessed TAG-seq data with appropriate FastP parameters to address issues identified in #26.</p>"},{"location":"sources/issues-index/#gsea-pathway-analysis","title":"GSEA &amp; Pathway Analysis","text":""},{"location":"sources/issues-index/#issue-41-stepwise-differential-abundance-approach","title":"Issue #41: Stepwise Differential Abundance Approach","text":"<p>Timeline: August 2025 Status: Completed Summary: Developed two-step approach: (1) Controls vs. treated, (2) Resistant vs. sensitive from step 1 genes.</p> <p>Implementation: <code>analyses/stepwise_differentialabundance/</code></p> <p>Results on Dataset 1: Only 1 significant gene. DESeq2 struggled with small gene set (~50 genes).</p> <p>Related: Stepwise notebook</p>"},{"location":"sources/issues-index/#issue-45-understand-gsea","title":"Issue #45: Understand GSEA","text":"<p>Timeline: August-September 2025 Status: Not completed Summary: Goal was to better understand and apply Gene Set Enrichment Analysis (GSEA) for pathway-level interpretation. Deferred.</p>"},{"location":"sources/issues-index/#classifier-development-validation","title":"Classifier Development &amp; Validation","text":""},{"location":"sources/issues-index/#issue-42-validate-sr320-classification-results","title":"Issue #42: Validate SR320 Classification Results","text":"<p>Timeline: August 2025 Status: In progress Summary: Validation of classification results. Question: Are the ~50 candidate markers convincing?</p> <p>Action needed: Make plots to assess marker quality.</p>"},{"location":"sources/issues-index/#issue-43-sr320s-ai-model","title":"Issue #43: SR320's AI Model","text":"<p>Timeline: August 2025 Status: Completed Summary: Development of machine learning classifier for phenotype prediction.</p>"},{"location":"sources/issues-index/#issue-44-combined-datasets-1-5","title":"Issue #44: Combined Datasets 1 &amp; 5","text":"<p>Timeline: August 2025 Status: \u2705 Completed - 6-gene classifier success Summary: Comparison of integrated data analysis vs. post-data integration approaches using datasets 1 and 5.</p> <p>Pipeline:</p> <ul> <li>Step 1: Rank genes by reproducibility, directionality consistency, and heterogeneity</li> <li>Step 2: Logistic regression for minimal gene set</li> </ul> <p>Result: 6-gene classifier panel with strong separation between tolerant and sensitive phenotypes.</p> <p>Key lesson: Only include training set in test set if exploring within study; for cross-study prediction, keep training and test separate (LOSO validation).</p> <p>Related: Gene classifier notebook</p>"},{"location":"sources/issues-index/#issue-47-details-unknown","title":"Issue #47: (Details Unknown)","text":"<p>Timeline: September 2025 Status: Not pursued Summary: No need to revisit.</p>"},{"location":"sources/issues-index/#issue-49-plot-6-genes-to-gain-confidence","title":"Issue #49: Plot 6 Genes to Gain Confidence","text":"<p>Timeline: September 2025 Status: In progress Summary: Visualization of 6-gene panel performance to assess how well genes distinguish phenotypes across studies.</p> <p>Related: Six-gene biomarker exploration</p>"},{"location":"sources/issues-index/#issue-51-replot-heatmap-with-improved-clustering","title":"Issue #51: Replot Heatmap with Improved Clustering","text":"<p>Timeline: September 2025 Status: To revisit Summary: Improve heatmap visualizations with better clustering algorithms and labels for clearer interpretation of gene panel performance.</p>"},{"location":"sources/issues-index/#issue-52-coverage-density-plots","title":"Issue #52: Coverage Density Plots","text":"<p>Timeline: September 2025 Status: Needs notebook entry Summary: Generate coverage density plots for quality control and validation. Still requires documentation in a notebook post.</p>"},{"location":"sources/issues-index/#issue-53-innate-vs-reactive-gene-expression","title":"Issue #53: Innate vs. Reactive Gene Expression","text":"<p>Timeline: September 2025 Status: Completed Summary: Critical analysis determining whether biomarkers are innate (constitutively different in controls) or reactive (induced by stress).</p> <p>Key insight: Biomarkers may exist in control groups if they represent innate resilience traits. The stepwise approach may remove these.</p> <p>Related: Innate gene expression notebook</p>"},{"location":"sources/issues-index/#literature-comparison","title":"Literature Comparison","text":""},{"location":"sources/issues-index/#issue-39-compare-de-results-from-papers","title":"Issue #39: Compare DE Results from Papers","text":"<p>Timeline: July 2025 Status: Remaining to be done Summary: Systematic comparison of project DEG results with published literature on oyster stress response. Create consolidated list of known DEGs/markers for validation.</p>"},{"location":"sources/issues-index/#navigation","title":"Navigation","text":"<p>Main Guide:</p> <ul> <li>Start Here - Field guide overview</li> <li>Timeline - Detailed project history</li> <li>Pipelines - Analysis workflows</li> </ul> <p>Other Sources:</p> <ul> <li>Notebook Posts Index - Analysis notebooks organized by theme</li> </ul>"},{"location":"sources/posts-index/","title":"Notebook Posts Index","text":"<p>This index organizes notebook posts from the project analysis notebook that document the biomarker discovery journey. Posts are grouped by theme and linked chronologically within each theme.</p> <p>All notebook posts are available at: <code>https://resilience-biomarkers-for-aquaculture.github.io/[post-slug]/</code></p>"},{"location":"sources/posts-index/#pipeline-development-infrastructure","title":"Pipeline Development &amp; Infrastructure","text":""},{"location":"sources/posts-index/#2024-09-10-rnaseq-workflow-with-reference-genome-es","title":"2024-09-10: RNAseq Workflow with Reference Genome (ES)","text":"<p>Timeline: September 2024 Summary: Initial RNAseq workflow setup using reference genome for C. gigas Dataset #1. Established baseline pipeline approach.</p>"},{"location":"sources/posts-index/#2024-09-11-rnaseq-workflow-with-de-novo-transcriptome","title":"2024-09-11: RNAseq Workflow with De Novo Transcriptome","text":"<p>Timeline: September 2024 Summary: Alternative approach using de novo transcriptome assembly for species without well-annotated reference genomes.</p>"},{"location":"sources/posts-index/#2024-09-25-set-up-nextflow-on-uw-klone-and-run-fetchngs","title":"2024-09-25: Set Up Nextflow on UW Klone and Run FetchNGS","text":"<p>Timeline: September 2024 Summary: Infrastructure setup for running nf-core pipelines on UW Klone HPC cluster. FetchNGS workflow for downloading public data.</p>"},{"location":"sources/posts-index/#2024-11-23-aws-batch-compute-environment-on-seqera-sy","title":"2024-11-23: AWS Batch Compute Environment on Seqera (SY)","text":"<p>Timeline: November 2024 Summary: Cloud-based compute infrastructure setup using Seqera platform and AWS Batch for scalable RNA-seq processing.</p>"},{"location":"sources/posts-index/#2024-12-03-repoint-nf-core-projectdir-to-srlab-directory-sw","title":"2024-12-03: Repoint nf-core projectDir to srlab Directory (SW)","text":"<p>Timeline: December 2024 Summary: Infrastructure configuration adjustments for consistent nf-core pipeline execution.</p>"},{"location":"sources/posts-index/#2025-01-06-creating-a-nextflow-pipeline-for-gene-count-analysis-sy","title":"2025-01-06: Creating a Nextflow Pipeline for Gene Count Analysis (SY)","text":"<p>Timeline: January 2025 Summary: Exploration of custom Nextflow pipeline development for gene count-based analyses beyond standard nf-core workflows.</p>"},{"location":"sources/posts-index/#differential-abundance-analysis","title":"Differential Abundance Analysis","text":""},{"location":"sources/posts-index/#2024-12-11-exploring-nf-core-differential-abundance-workflow-es","title":"2024-12-11: Exploring nf-core Differential Abundance Workflow (ES)","text":"<p>Timeline: December 2024 Related: Timeline - Dec 4, 2024 Summary: Initial exploration of nf-core/differentialabundance pipeline. Encountered compute resource limits when attempting to process 4 combined datasets.</p> <p>Key lesson: Integration of heterogeneous datasets requires careful consideration of batch effects and nuances.</p>"},{"location":"sources/posts-index/#2025-07-01-run-nf-core-differentialabundance-pipeline-on-c-virg-data-sw","title":"2025-07-01: Run nf-core Differentialabundance Pipeline on C. virg Data (SW)","text":"<p>Timeline: July 2025 Related: Issue #34, Timeline - July 2025 Summary: Per-dataset differential abundance analysis. Combined Study 4 (injected group) + Study 5 to increase sample size.</p> <p>Key learning: Understanding normalization timing in the pipeline:</p> <ul> <li>PCAs generated before differential abundance analysis</li> <li>Normalization (VST, rlog) happens during DESeq2</li> <li>Batch correction applied post-normalization</li> </ul> <p>Observation: Started seeing evidence of innate trait in batch-corrected PCAs on top 500 most variable genes.</p>"},{"location":"sources/posts-index/#2025-08-05-creating-a-gmt-file-for-use-with-gsea-sy","title":"2025-08-05: Creating a GMT File for Use with GSEA (SY)","text":"<p>Timeline: August 2025 Related: Issue #41 Summary: Integrating Gene Set Enrichment Analysis (GSEA) with differential abundance pipeline. Created GMT file with gene descriptions for pathway-level interpretation.</p>"},{"location":"sources/posts-index/#2025-08-26-step-wise-differential-abundance-with-cvirg-dataset-1-sw","title":"2025-08-26: Step-wise Differential Abundance with Cvirg Dataset 1 (SW)","text":"<p>Timeline: August 2025 Related: Issue #41, Stepwise Pipeline Summary: Implementation of two-step filtering approach:</p> <ol> <li>Step 1: Controls vs. treated (identify stress-responsive genes)</li> <li>Step 2: Resistant vs. sensitive (from Step 1 genes)</li> </ol> <p>Results: Only 1 significant gene identified in Step 2</p> <p>Problem: DESeq2's VST couldn't work properly with highly filtered gene set (~50 genes)</p> <p>Key insight: Raised question about whether stepwise filtering removes innate biomarkers present in control samples.</p>"},{"location":"sources/posts-index/#data-processing-quality-control","title":"Data Processing &amp; Quality Control","text":""},{"location":"sources/posts-index/#2024-11-04-run-rnaseq-on-perkinsus-datasets-sw","title":"2024-11-04: Run RNAseq on Perkinsus Datasets (SW)","text":"<p>Timeline: November 2024 Summary: Processing and quality control of Perkinsus marinus challenge datasets.</p>"},{"location":"sources/posts-index/#2024-12-13-compare-genes-after-cgi-id-conversion-attempt-1-sw","title":"2024-12-13: Compare Genes After CGI ID Conversion Attempt 1 (SW)","text":"<p>Timeline: December 2024 Summary: Gene identifier harmonization across datasets with different annotation versions. Critical for cross-dataset comparisons.</p>"},{"location":"sources/posts-index/#2024-12-23-pca-for-gene-count-comparison-between-ncbi-annotation-releases-sy","title":"2024-12-23: PCA for Gene Count Comparison Between NCBI Annotation Releases (SY)","text":"<p>Timeline: December 2024 Summary: Assessed impact of different C. gigas annotation releases on gene count matrices and downstream analysis using PCA.</p>"},{"location":"sources/posts-index/#2025-04-15-reprocess-tag-seq-data-with-different-fastp-parameters-sw","title":"2025-04-15: Reprocess TAG-seq Data with Different FastP Parameters (SW)","text":"<p>Timeline: April 2025 Related: Issue #26, Issue #28, Timeline - April 2025 Summary: Critical technical discovery: Johnson dataset used TAG-seq (not standard RNA-seq). Initial analysis with standard RNA-seq parameters was inappropriate.</p> <p>Issues:</p> <ul> <li>GC bias detected</li> <li>Wrong adapter trimming parameters</li> <li>Inappropriate quality filtering</li> </ul> <p>Solution: Reran with TAG-seq-specific FastP parameters, improving data quality.</p>"},{"location":"sources/posts-index/#gene-classifier-biomarker-development","title":"Gene Classifier &amp; Biomarker Development","text":""},{"location":"sources/posts-index/#2025-09-11-two-script-pipeline-for-gene-expression-classifier-sy","title":"2025-09-11: Two-Script Pipeline for Gene-Expression Classifier (SY)","text":"<p>Timeline: August-September 2025 Status: \u2705 Core methodology for 6-gene classifier Related: Issue #44, Two-Step Classifier Pipeline Summary: Landmark analysis - Developed validated two-step classifier approach combining datasets 1 &amp; 5.</p> <p>Methodology:</p> <ul> <li>Step 1 (R script): Meta-analysis ranking genes by reproducibility, effect consistency, and heterogeneity across studies</li> <li>Step 2 (Python script): Logistic regression with LASSO regularization for minimal gene panel; LOSO validation</li> </ul> <p>Result: 6-gene classifier with strong separation between tolerant/sensitive phenotypes</p> <p>Key innovation: Explicitly separates feature discovery from classifier construction to reduce overfitting across batches</p> <p>Code: Two-step pipeline repository</p> <p>ChatGPT session: Development discussion</p>"},{"location":"sources/posts-index/#2025-10-01-series-of-deseq2-runs-indicates-innate-degs-for-tolerance-sy","title":"2025-10-01: Series of DESeq2 Runs Indicates Innate DEGs for Tolerance (SY)","text":"<p>Timeline: September 2025 Related: Issue #53, Validation &amp; Pitfalls Summary: Critical biological insight - Discovered that differential expression between tolerant/sensitive samples is relatively independent of treatment status.</p> <p>Finding: Biomarkers appear to be innate (constitutively different between families) rather than reactive (induced by stress)</p> <p>Implication: Stepwise filtering approach (control vs. treated first) would remove these innate biomarkers</p> <p>Approach:</p> <ul> <li>Used Day 7 samples from Studies 1 &amp; 5 (most compatible designs)</li> <li>Ran contrasts on: Treatment, Treated-only samples, Control-only samples, Combined samples</li> <li>Treatment contrast: only 1 DEG</li> <li>Treated-only contrast: only 1 DEG</li> <li>Control and treated samples showed similar tolerant vs. sensitive DEGs</li> </ul> <p>Conclusion: Tolerance/sensitivity is an innate differential expression profile, common to control and treated samples</p>"},{"location":"sources/posts-index/#2025-10-01-exploring-six-gene-biomarker-across-studies-sy","title":"2025-10-01: Exploring Six-Gene Biomarker Across Studies (SY)","text":"<p>Timeline: September 2025 Related: Issue #49 Summary: Visualization and validation of the 6-gene classifier panel across multiple studies. Assesses how well genes distinguish phenotypes in independent datasets.</p>"},{"location":"sources/posts-index/#2026-01-05-common-genes-per-loso-fold-sy","title":"2026-01-05: Common Genes per LOSO Fold (SY)","text":"<p>Timeline: January 2026 Summary: Analysis of gene consistency across Leave-One-Study-Out (LOSO) cross-validation folds. Identifies genes that are robustly selected regardless of which study is held out.</p>"},{"location":"sources/posts-index/#exploratory-supporting-analyses","title":"Exploratory &amp; Supporting Analyses","text":""},{"location":"sources/posts-index/#2024-12-31-using-chatgpt-to-explore-thermal-resilience-based-on-gene-counts-sy","title":"2024-12-31: Using ChatGPT to Explore Thermal Resilience Based on Gene Counts (SY)","text":"<p>Timeline: December 2024 Summary: Exploratory use of AI tools (ChatGPT) for hypothesis generation about thermal tolerance based on gene expression patterns. Documents AI-assisted analysis workflow.</p>"},{"location":"sources/posts-index/#methylation-sequencing","title":"Methylation Sequencing","text":""},{"location":"sources/posts-index/#2024-11-27-run-methylseq-on-klone-sw","title":"2024-11-27: Run Methylseq on Klone (SW)","text":"<p>Timeline: November 2024 Summary: Epigenetic analysis using nf-core/methylseq pipeline on C. virginica whole-genome bisulfite sequencing (WGBS) data.</p>"},{"location":"sources/posts-index/#2025-04-07-reproduce-oyster-wgbs-analysis-with-nf-core-methylseq-sw","title":"2025-04-07: Reproduce Oyster WGBS Analysis with nf-core Methylseq (SW)","text":"<p>Timeline: April 2025 Summary: Validation of methylseq pipeline results by comparing with published oyster WGBS analysis.</p>"},{"location":"sources/posts-index/#2025-05-02-run-c-virginica-wgbs-data-through-nf-core-methylseq-sw","title":"2025-05-02: Run C. virginica WGBS Data Through nf-core Methylseq (SW)","text":"<p>Timeline: May 2025 Summary: Processing additional C. virginica WGBS datasets for methylation biomarker discovery.</p>"},{"location":"sources/posts-index/#2025-08-06-run-methylseq-on-m-chilensis-wgbs-data-sw","title":"2025-08-06: Run Methylseq on M. chilensis WGBS Data (SW)","text":"<p>Timeline: August 2025 Summary: Expansion to Chilean blue mussel (Mytilus chilensis) methylation data to assess cross-species epigenetic patterns.</p>"},{"location":"sources/posts-index/#thematic-groups-summary","title":"Thematic Groups Summary","text":""},{"location":"sources/posts-index/#pipeline-development-6-posts","title":"\ud83d\udd27 Pipeline Development (6 posts)","text":"<p>Posts focused on infrastructure, workflow setup, and computational resource configuration.</p>"},{"location":"sources/posts-index/#differential-abundance-4-posts","title":"\ud83d\udcca Differential Abundance (4 posts)","text":"<p>Posts documenting differential expression analysis attempts, including integrated, per-dataset, and stepwise approaches.</p>"},{"location":"sources/posts-index/#classifier-biomarkers-4-posts","title":"\ud83c\udfaf Classifier &amp; Biomarkers (4 posts)","text":"<p>Posts detailing the development and validation of the 6-gene classifier, including innate vs. reactive characterization.</p>"},{"location":"sources/posts-index/#data-qc-processing-4-posts","title":"\ud83d\udd2c Data QC &amp; Processing (4 posts)","text":"<p>Posts addressing data quality, annotation harmonization, and technical parameter optimization (especially TAG-seq).</p>"},{"location":"sources/posts-index/#methylation-4-posts","title":"\ud83e\uddec Methylation (4 posts)","text":"<p>Posts exploring epigenetic biomarkers using WGBS data.</p>"},{"location":"sources/posts-index/#chronological-navigation","title":"Chronological Navigation","text":"<p>2024 Posts: September | November | December</p> <p>2025 Posts: January | April | May | July-October</p> <p>All Posts: Archive by Date</p>"},{"location":"sources/posts-index/#navigation","title":"Navigation","text":"<p>Main Guide:</p> <ul> <li>Start Here - Field guide overview</li> <li>Timeline - Detailed project history organized by month</li> <li>Pipelines - Validated analysis workflows</li> </ul> <p>Other Sources:</p> <ul> <li>GitHub Issues Index - Issues organized by theme</li> </ul>"}]}